import argparse
import random
from transformers import GPT2TokenizerFast, GPT2Model
import numpy as np
from collections import Counter
import torch
import torch.nn.functional as F

def analyze_character_importance(text, tokenizer):
    """
    Analyze which characters are most important for tokenization
    """
    # Tokenize the original text
    original_tokens = tokenizer.tokenize(text)
    
    importance_scores = {}
    
    # Test removing each character individually
    for i, char in enumerate(text):
        if char.strip():  # Only consider non-space characters
            # Create text with this character removed
            modified_text = text[:i] + text[i+1:]
            modified_tokens = tokenizer.tokenize(modified_text)
            
            # Calculate tokenization difference
            token_diff = len(modified_tokens) - len(original_tokens)
            
            # Store importance (higher diff = more important character)
            importance_scores[char] = importance_scores.get(char, 0) + abs(token_diff)
    
    return importance_scores

def find_critical_characters(text, tokenizer, top_n=10):
    """
    Find the characters that most affect tokenization
    """
    importance = analyze_character_importance(text, tokenizer)
    
    # Sort by importance
    sorted_chars = sorted(importance.items(), key=lambda x: x[1], reverse=True)
    
    print("Most important characters for tokenization:")
    for char, score in sorted_chars[:top_n]:
        print(f"'{char}': {score}")
    
    return [char for char, _ in sorted_chars[:top_n]]

def selective_replacement(text, replacement_rate=0.5, aggression=0.9):
    """
    Replace common characters with random alphabets and uncommon characters with homoglyphs
    """
    # English letter frequency (most to least common)
    common_chars = 'etaoinshrdlcumwfgypbvkjxqz'
    random_alphabets = 'qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM'
    
    # Homoglyph mapping for uncommon characters
    homoglyph_map = {
        'x': 'х', 'z': 'ᴢ', 'q': 'զ', 'j': 'ј', 'k': 'κ',
        'v': 'ν', 'w': 'ԝ', 'f': 'ғ', 'b': 'Ь', 'p': 'р',
        'g': 'ɡ', 'y': 'у', 'm': 'ｍ', 'u': 'υ', 'c': 'с'
    }
    
    result = []
    for char in text:
        char_lower = char.lower()
        
        if char_lower in common_chars and char_lower.isalpha():
            # Common character - replace with random alphabet based on aggression
            if random.random() < replacement_rate * aggression:
                random_char = random.choice(random_alphabets)
                result.append(random_char)
            else:
                result.append(char)
        elif char_lower in homoglyph_map and char_lower.isalpha():
            # Uncommon character - replace with homoglyph based on aggression
            if random.random() < aggression * 0.9:  # Higher chance for uncommon chars
                replacement = homoglyph_map[char_lower]
                result.append(replacement.upper() if char.isupper() else replacement)
            else:
                result.append(char)
        else:
            # Preserve spaces, punctuation, and other characters
            result.append(char)
    
    return ''.join(result)

def frequency_based_mixed_replacement(text, aggression=0.9):
    """
    Replace characters based on frequency: common ones with random alphabets, uncommon ones with homoglyphs
    """
    # English letter frequency (most to least common)
    english_frequency = 'etaoinshrdlcumwfgypbvkjxqz'
    random_alphabets = 'qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM'
    
    # Homoglyphs for less common characters
    homoglyph_map = {
        'z': 'ᴢ', 'q': 'զ', 'x': 'х', 'j': 'ј', 'k': 'κ',
        'v': 'ν', 'w': 'ԝ', 'f': 'ғ', 'b': 'Ь', 'p': 'р',
        'g': 'ɡ', 'y': 'у', 'm': 'ｍ', 'u': 'υ', 'c': 'с'
    }
    
    result = []
    for char in text:
        char_lower = char.lower()
        
        if char_lower in english_frequency and char_lower.isalpha():
            frequency_rank = english_frequency.index(char_lower)
            
            if frequency_rank < 8:  # Top 8 most common characters
                # Very common - high chance of random alphabet replacement
                if random.random() < aggression * 0.95:
                    random_char = random.choice(random_alphabets)
                    result.append(random_char)
                else:
                    result.append(char)
            elif frequency_rank < 15:  # Middle frequency
                # Moderate chance of replacement
                if random.random() < aggression * 0.8:
                    if char_lower in homoglyph_map:
                        replacement = homoglyph_map[char_lower]
                        result.append(replacement.upper() if char.isupper() else replacement)
                    else:
                        random_char = random.choice(random_alphabets)
                        result.append(random_char)
                else:
                    result.append(char)
            else:  # Less common characters
                # High chance of homoglyph replacement
                if random.random() < aggression * 0.9:
                    if char_lower in homoglyph_map:
                        replacement = homoglyph_map[char_lower]
                        result.append(replacement.upper() if char.isupper() else replacement)
                    else:
                        random_char = random.choice(random_alphabets)
                        result.append(random_char)
                else:
                    result.append(char)
        else:
            result.append(char)
    
    return ''.join(result)

def vowel_preservation_replacement(text, aggression=0.8):
    """
    Preserve vowels but replace consonants with random alphabets or homoglyphs
    """
    vowels = {'a', 'e', 'i', 'o', 'u', 'y'}
    random_alphabets = 'qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM'
    
    # Homoglyphs for certain consonants
    consonant_homoglyphs = {
        'b': 'Ь', 'c': 'с', 'd': 'ԁ', 'f': 'ғ', 'g': 'ɡ',
        'h': 'һ', 'j': 'ј', 'k': 'κ', 'l': 'ӏ', 'm': 'ｍ',
        'n': 'ո', 'p': 'р', 'q': 'զ', 'r': 'г', 's': 'ѕ',
        't': 'т', 'v': 'ν', 'w': 'ԝ', 'x': 'х', 'z': 'ᴢ'
    }
    
    result = []
    for char in text:
        char_lower = char.lower()
        
        if char_lower in vowels:
            # Preserve vowels for readability
            result.append(char)
        elif char_lower.isalpha():
            # Consonant - replace with random alphabet or homoglyph
            if random.random() < aggression:
                if char_lower in consonant_homoglyphs and random.random() < 0.7:
                    # Use homoglyph for certain consonants
                    replacement = consonant_homoglyphs[char_lower]
                    result.append(replacement.upper() if char.isupper() else replacement)
                else:
                    # Use random alphabet
                    random_char = random.choice(random_alphabets)
                    result.append(random_char)
            else:
                result.append(char)
        else:
            result.append(char)
    
    return ''.join(result)

def token_boundary_mixed_attack(text, aggression=0.9):
    """
    Attack token boundaries with mixed replacement strategy
    """
    words = text.split()
    poisoned_words = []
    
    for word in words:
        if len(word) <= 2:
            poisoned_words.append(word)
            continue
        
        # Preserve first and last characters
        first_char = word[0]
        last_char = word[-1] if word[-1].isalpha() else word[-2] if len(word) > 1 else word[0]
        
        # Attack the middle of the word with mixed strategy
        middle = word[1:-1] if len(word) > 2 else ""
        
        if middle and random.random() < aggression:
            poisoned_middle = frequency_based_mixed_replacement(middle, aggression=aggression)
            poisoned_word = first_char + poisoned_middle + last_char
        else:
            poisoned_word = word
        
        poisoned_words.append(poisoned_word)
    
    return " ".join(poisoned_words)

# --- SEMANTIC DISRUPTION APPROACH ---
def poison_semantic_disruption(text, aggression=0.9):
    """
    Disrupt semantic meaning while maintaining some visual similarity
    """
    # Dictionary of word replacements that change meaning
    semantic_disruptions = {
        'Python': ['Pythön', 'Pyton', 'Pythôn', 'Pyth0n', 'Pythοn'],
        'is': ['ís', 'ïѕ', 'іѕ', 'iѕ', 'іs'],
        'a': ['á', 'а', 'ɑ', 'а́'],
        'popular': ['populаr', 'pοpular', 'populâr', 'p0pular'],
        'programming': ['progrаmming', 'programming', 'progrаmming', 'prοgramming'],
        'language': ['lаnguage', 'languаge', 'langυage', 'lаnguage'],
        'the': ['thе', 'tһe'],
        'to': ['tο', 'tо'],
        'and': ['аnd', 'anԁ'],
        'for': ['fοr', 'fоr'],
        'with': ['wіth', 'witһ'],
    }
    
    # Additional disruptive transformations
    disruptive_transforms = [
        lambda s: s.replace('ing', 'inɡ').replace('ed', 'еd'),
        lambda s: s.replace('th', 'tһ').replace('sh', 'ѕh'),
        lambda s: s.replace('oo', 'οο').replace('ee', 'ее'),
        lambda s: s + '\u200B\u200C\u200D',  # Add invisible suffix
        lambda s: '\u2060\u2062\u2063' + s,  # Add invisible prefix
    ]
    
    words = text.split()
    disrupted_words = []
    
    # Apply semantic disruptions
    for word in words:
        clean_word = word.strip('.,!?;:')
        punctuation = word[len(clean_word):] if word != clean_word else ''
        
        if clean_word in semantic_disruptions and random.random() < aggression:
            replacement = random.choice(semantic_disruptions[clean_word])
            disrupted_words.append(replacement + punctuation)
        else:
            disrupted_words.append(word)
    
    # Reconstruct text and apply additional transforms
    poisoned_text = " ".join(disrupted_words)
    
    for transform in disruptive_transforms:
        if random.random() < aggression:
            poisoned_text = transform(poisoned_text)
    
    return poisoned_text

# --- TOKEN BOUNDARY ATTACK ---
def poison_token_boundaries(text, aggression=0.9):
    """
    Specifically attack token boundaries that GPT-2 uses
    """
    # Replace spaces with various zero-width combinations
    space_replacements = [
        ' \u200B\u200C\u200D ',
        ' \u2060\u2062\u2063 ',
        ' \u200B\u2060 ',
        ' \u200C\u2062 ',
        ' \u200D\u2063 ',
    ]
    
    # Attack common token prefixes/suffixes
    boundary_attacks = {
        'Py': 'P\u200By',
        'th': 't\u200Bh',
        'on': 'o\u200Bn',
        'is': 'i\u200Bs',
        'pop': 'p\u200Bo\u200Bp',
        'pro': 'p\u200Br\u200Bo',
        'gram': 'g\u200Br\u200Ba\u200Bm',
        'lang': 'l\u200Ba\u200Bn\u200Bg',
        'uage': 'u\u200Ba\u200Bg\u200Be',
    }
    
    # Apply space replacements
    for replacement in space_replacements:
        if random.random() < aggression:
            text = text.replace(' ', replacement)
    
    # Apply boundary attacks
    for pattern, attack in boundary_attacks.items():
        if random.random() < aggression:
            text = text.replace(pattern, attack)
    
    return text

def combined_poison(text, aggression=0.9):
    """Combine multiple poisoning strategies"""
    # Apply semantic disruption first
    poisoned = poison_semantic_disruption(text, aggression)
    # Then apply token boundary attacks
    poisoned = poison_token_boundaries(poisoned, aggression)
    # Finally apply frequency-based replacement
    poisoned = frequency_based_mixed_replacement(poisoned, aggression)
    return poisoned

def get_embedding(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
    hidden_states = outputs.last_hidden_state
    return hidden_states.mean(dim=1).squeeze(0)

def cosine_sim(a, b):
    return F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()

def calculate_readability(text):
    """
    Simple readability heuristic (0-1, higher is more readable)
    """
    # Count preserved vowels and word boundaries
    vowels = {'a', 'e', 'i', 'o', 'u', 'y'}
    word_chars = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')
    
    vowel_count = sum(1 for char in text if char.lower() in vowels)
    word_char_count = sum(1 for char in text if char in word_chars)
    space_count = text.count(' ')
    
    if word_char_count == 0:
        return 0
    
    # Count random alphabet characters (less negative impact than junk chars)
    random_alpha_count = sum(1 for char in text if char in 'qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM')
    
    # Count zero-width characters (negative impact)
    zw_chars = {'\u200B', '\u200C', '\u200D', '\u2060', '\u2062', '\u2063'}
    zw_count = sum(1 for char in text if char in zw_chars)
    
    # Simple heuristic: ratio of vowels minus random alphabet impact
    readability = (vowel_count / word_char_count) * 0.6 
    readability += (space_count / max(1, len(text.split()))) * 0.4
    readability -= (random_alpha_count / len(text)) * 0.2  # Less penalty for random alphabets
    readability -= (zw_count / len(text)) * 0.3  # Penalty for zero-width chars
    
    return max(0, min(1.0, readability))

def find_effective_poison(text, tokenizer, model, target=0.8, attempts=50):
    """
    Try to find poisoning that actually affects embeddings
    """
    clean_emb = get_embedding(text, tokenizer, model)
    best_sim = 1.0
    best_poison = text
    
    strategies = [
        ("Semantic Disruption", lambda: poison_semantic_disruption(text, 0.9)),
        ("Token Boundary", lambda: poison_token_boundaries(text, 0.9)),
        ("Frequency-based", lambda: frequency_based_mixed_replacement(text, 0.9)),
        ("Combined", lambda: combined_poison(text, 0.8)),
        ("Selective Replacement", lambda: selective_replacement(text, 0.5, 0.9)),
    ]
    
    print("Searching for effective poison...")
    
    for attempt in range(attempts):
        strategy_name, strategy = random.choice(strategies)
        poisoned = strategy()
        
        try:
            emb_poison = get_embedding(poisoned, tokenizer, model)
            sim = cosine_sim(clean_emb, emb_poison)
            
            if sim < best_sim:
                best_sim = sim
                best_poison = poisoned
                print(f"Attempt {attempt} ({strategy_name}): {sim:.4f}")
            
            if sim <= target:
                break
                
        except Exception as e:
            continue
    
    return best_poison, best_sim

def analyze_embedding_robustness(text, poisoned, tokenizer, model):
    """
    Analyze why GPT-2 embeddings are so robust
    """
    clean_emb = get_embedding(text, tokenizer, model)
    poisoned_emb = get_embedding(poisoned, tokenizer, model)
    
    # Calculate similarity metrics
    cosine = cosine_sim(clean_emb, poisoned_emb)
    euclidean = torch.dist(clean_emb, poisoned_emb).item()
    
    # Check if embeddings are normalized
    clean_norm = torch.norm(clean_emb).item()
    poisoned_norm = torch.norm(poisoned_emb).item()
    
    print(f"Cosine similarity: {cosine:.6f}")
    print(f"Euclidean distance: {euclidean:.6f}")
    print(f"Clean norm: {clean_norm:.4f}, Poisoned norm: {poisoned_norm:.4f}")
    
    if cosine > 0.95:
        print("GPT-2 embeddings are extremely robust to character-level changes!")
        print("This suggests that:")
        print("1. The tokenizer handles homoglyphs and ZW chars robustly")
        print("2. The embedding layer may normalize inputs")
        print("3. Mean pooling smooths out local perturbations")
    
    return cosine

def test_replacement_strategies(text, target_similarity=0.8):
    """
    Test different replacement strategies
    """
    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
    model = GPT2Model.from_pretrained("gpt2")
    model.eval()
    
    clean_emb = get_embedding(text, tokenizer, model)
    
    strategies = [
        ("Selective Alphabet Replacement", lambda: selective_replacement(text, 0.5, 0.9)),
        ("Frequency-based Mixed Replacement", lambda: frequency_based_mixed_replacement(text, 0.9)),
        ("Vowel Preservation with Alphabet Replacement", lambda: vowel_preservation_replacement(text, 0.8)),
        ("Token Boundary Mixed Attack", lambda: token_boundary_mixed_attack(text, 0.9)),
        ("Semantic Disruption", lambda: poison_semantic_disruption(text, 0.9)),
        ("Token Boundary Attack", lambda: poison_token_boundaries(text, 0.9)),
        ("Combined Strategy", lambda: combined_poison(text, 0.8)),
    ]
    
    results = []
    
    for name, strategy in strategies:
        poisoned = strategy()
        try:
            emb_poison = get_embedding(poisoned, tokenizer, model)
            sim = cosine_sim(clean_emb, emb_poison)
            
            # Calculate human readability score
            readability = calculate_readability(poisoned)
            
            results.append({
                'strategy': name,
                'poisoned_text': poisoned,
                'similarity': sim,
                'readability': readability,
                'success': sim <= target_similarity,
                'length': len(poisoned)
            })
            
            print(f"{name}: similarity={sim:.4f}, readability={readability:.2f}, length={len(poisoned)}")
            
        except Exception as e:
            print(f"{name}: Error - {e}")
    
    return results

def demo_replacement_strategies():
    text = "Python is a popular programming language."
    target = 0.8
    
    print("=== COMPREHENSIVE POISONING STRATEGIES ===")
    print(f"Target similarity: {target}")
    print(f"Clean text: {text} (length: {len(text)})")
    print("\n" + "="*70)
    
    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
    model = GPT2Model.from_pretrained("gpt2")
    model.eval()
    
    # Find the most effective poison
    best_poison, best_sim = find_effective_poison(text, tokenizer, model, target)
    
    print("\n--- ROBUSTNESS ANALYSIS ---")
    analyze_embedding_robustness(text, best_poison, tokenizer, model)
    
    print("\n--- TOKENIZATION ANALYSIS ---")
    print("Clean tokens:", tokenizer.tokenize(text))
    print("Poisoned tokens:", tokenizer.tokenize(best_poison))
    
    # Test all strategies
    results = test_replacement_strategies(text, target)
    
    print("\n=== RESULTS ===")
    for result in results:
        status = "✅" if result['success'] else "❌"
        print(f"{status} {result['strategy']}:")
        print(f"   Similarity: {result['similarity']:.4f}")
        print(f"   Readability: {result['readability']:.2f}")
        print(f"   Length: {result['length']}")
        print(f"   Text: {result['poisoned_text']}")
        print()

if __name__ == "__main__":
    demo_replacement_strategies()