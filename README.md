GPT-2 Text Poisoning Experiment
A fun experiment exploring text “poisoning” for GPT-2 embeddings. This project applies small semantic changes, token-boundary manipulations, and invisible characters to see how robust GPT-2 embeddings really are.

Getting ~0.8885 in GPT-2

According to Claude its around:
Token-level similarity: Probably 0.4-0.6
Character n-gram similarity: Maybe 0.3-0.5
Semantic embedding similarity: Possibly 0.6-0.8, since the meaning, imagery, and poetic nature are preserved

It can go lower if different tweaks are tried
