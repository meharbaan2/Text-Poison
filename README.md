GPT-2 Text Poisoning Experiment
A fun experiment exploring text “poisoning” for GPT-2 embeddings. This project applies small semantic changes, token-boundary manipulations, and invisible characters to see how robust GPT-2 embeddings really are.

So far I can get it down to about 0.9 cosine. Just exploring and seeing how robust these embeddings really are.
